{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Spark and PySpark Basics\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Install The Spark and Hadoop together"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "from \n",
    "https://medium.com/@josemarcialportilla/getting-spark-python-and-jupyter-notebook-running-on-amazon-ec2-dec599e1c297"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "okay so from the instructions above, I wanted to do without anaconda and also I wanted to do it in container:\n",
    "\n",
    "\n",
    "So these basic changes\n",
    "\n",
    "```\n",
    "ENV SPARK_HOME='/spark-2.0.0-bin-hadoop2.7' \n",
    "ENV   PATH=$SPARK_HOME:$PATH \n",
    "ENV  PYTHONPATH=$SPARK_HOME/python:$PYTHONPATH\n",
    "```\n",
    "\n",
    "also [here](https://stackoverflow.com/questions/33379393/docker-env-vs-run-export) tells how to use export type commands in dockerfile\n",
    "\n",
    "but I got this error that with some search I realized that pyspark is not supporting the python 3.6 and above\n",
    "\n",
    "https://stackoverflow.com/questions/42349980/unable-to-run-pyspark\n",
    "\n",
    "https://issues.apache.org/jira/browse/SPARK-19019\n",
    "\n",
    "\n",
    "So from here my thought was that okay I will switch to the python 3.5\n",
    "\n",
    "(going back to the link above it seems that they claim to have fixed that in a later version of pyspark, so this is another route that I could take\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## install later version of pyspark"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "So taking a fresh look of this and seeing that the issue has been old and calimed to be resolved, I come to this basic idea \n",
    "that I could simply use a later version of spark, even better why not just find a later more credible source for installation.\n",
    "\n",
    "for example the spark official site!\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/getting_started/install.html\n",
    "\n",
    "Which says you could simply use pip install for that !!!\n",
    "\n",
    "Now since I had so much pain trying to figure this puzzle out, I would not simply go for that option, at least not just yet!\n",
    "plus not sure how flexible is that pip install option, so why not continue with manual installation, huh?!!\n",
    "\n",
    "https://spark.apache.org/downloads.html\n",
    "\n",
    "\n",
    "which gives me this link for the latest version \n",
    "\n",
    "https://www.apache.org/dyn/closer.lua/spark/spark-3.1.1/spark-3.1.1-bin-hadoop2.7.tgz\n",
    "\n",
    "which links to this \n",
    "\n",
    "https://downloads.apache.org/spark/spark-3.1.1/spark-3.1.1-bin-hadoop2.7.tgz\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "and it works!\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Simnply use pip install!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Install python 3.5\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "okay I wanted to simply install the python 3.5 using the apt-get, should be pretty simple huh? except it wasn't!\n",
    "well I with a lot of different suggestions I could get python 3.5 installed, but it seemed that it lacked zlib\n",
    "which the question of how to install python 3.5 with zlib was something I was struggling for long!\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## This was local install only!\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "This is usually for local usage or as a client to connect to a cluster instead of setting up a cluster itself.\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/getting_started/install.html"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Getting Started with PySpark\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "okay so after setting up the pyspark from the apache pyspark home, there are a bunch of good tutorials there that one\n",
    "would want to go through \n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/user_guide/arrow_pandas.html\n",
    "\n",
    "\n",
    "https://github.com/apache/spark/tree/v3.1.1-rc3/examples/src/main/python\n",
    "\n",
    "\n",
    "https://spark.apache.org/community.html\n",
    "\n",
    "http://spark.apache.org/docs/latest/quick-start.html\n",
    "\n",
    "\n",
    "http://spark.apache.org/docs/latest/index.html#where-to-go-from-here\n",
    "\n",
    "\n",
    "This part is the part that I should be doing, but I'll come back to this!\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Setup a Spark Cluster"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Similar to the experiment with the Hadoop cluster, I was keen to see how I could set up a Spark cluster.\n",
    "\n",
    "from what I was going through it was not clear if the installation of spark will entail the Hadoop as well, or you would need to set it up on top of an existing hadoop cluster, or what \n",
    "I was keen to set that up all in docker or possibly make use of that previous hadoop cluster in docker I had.\n",
    "\n",
    "\n",
    "\n",
    "https://spark.apache.org/docs/latest/spark-standalone.html\n",
    "\n",
    "This is for stand alone installation. Not clear what exaclty is a stand alone.\n",
    "Looks like you could use these stand alones to use on a single machine or to build clusters (without a separate hadoop(?))\n",
    "\n",
    "> In addition to running on the Mesos or YARN cluster managers, Spark also provides a simple standalone deploy mode. You can launch a standalone cluster either manually, by starting a master and workers by hand, or use our provided launch scripts. It is also possible to run these daemons on a single machine for testing.\n",
    "\n",
    "> To install Spark Standalone mode, you simply place a compiled version of Spark on each node on the cluster. You can obtain pre-built versions of Spark with each release or build it yourself.\n",
    "\n",
    "Well this seemed to be the simplest of the spark cluster installation options that was there. Still too complicated.\n",
    "\n",
    "\n",
    "https://spark.apache.org/docs/latest/cluster-overview.html\n",
    "\n",
    "This gave a overview of how one could setup spark cluster.\n",
    "\n",
    "\n",
    "\n",
    "This here also included Yarn and Kubernetes options which seemed really interesting, but going through them seemed even more complicated than the stand alone\n",
    "\n",
    "\n",
    "Also one interesting option that I think may come back to later is\n",
    "Using Spark's \"Hadoop Free\" Build\n",
    "\n",
    "http://spark.apache.org/docs/latest/hadoop-provided.\n",
    "\n",
    "\n",
    "\n",
    "okay then I also campe across this tutorial for spark cluster setup\n",
    "Apache Spark Cluster Setup\n",
    "\n",
    "https://www.tutorialkart.com/apache-spark/how-to-setup-an-apache-spark-cluster/\n",
    "\n",
    "WHICH is supposed be step by step. very good, yet still not quite what I am looking.\n",
    "\n",
    "Next I found a better path. A cluster setup with docker file and docker compose. SO here it goes next\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Spark Cluster with Docker"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "from https://www.kdnuggets.com/2020/07/apache-spark-cluster-docker.html\n",
    "\n",
    "\n",
    "Build your own Apache Spark cluster in standalone mode on Docker with a JupyterLab interface.\n",
    "\n",
    "To get started, you can run Apache Spark on your machine by using one of the many great Docker distributions available out there. Jupyter offers an excellent dockerized Apache Spark with a JupyterLab interface but misses the framework distributed core by running it on a single container.\n",
    "\n",
    " By the end, you will have a fully functional Apache Spark cluster built with Docker and shipped with a Spark master node, two Spark worker nodes and a JupyterLab interface. It will also include the Apache Spark Python API (PySpark) and a simulated Hadoop distributed file system (HDFS).\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "As mentioned, we need to create, build and compose the Docker images for JupyterLab and Spark nodes to make the cluster. We will use the following Docker image hierarchy:\n",
    "\n",
    "The cluster base image will download and install common software tools (Java, Python, etc.) and will create the shared directory for the HDFS. On the Spark base image, the Apache Spark application will be downloaded and configured for both the master and worker nodes. The Spark master image will configure the framework to run as a master node. Similarly, the Spark worker node will configure Apache Spark application to run as a worker node. Finally, the JupyterLab image will use the cluster base image to install and configure the IDE and PySpark, Apache Sparkâ€™s Python API.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## the cluster base image\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Furthermore, we will get an Apache Spark version with Apache Hadoop support to allow the cluster to simulate the HDFS using the shared volume created in the base cluster image.\n",
    "\n",
    "Letâ€™s start by downloading the Apache Spark latest version (currently 3.0.0) with Apache Hadoop support from the official Apache repository. Then, we play a bit with the downloaded package (unpack, move, etc.) and we are ready for the setup stage. Lastly, we configure four Spark variables common to both master and workers nodes:\n",
    "\n",
    "SPARK_HOME is the installed Apache Spark location used by the framework for setting up tasks;\n",
    "SPARK_MASTER_HOST is the master node hostname used by worker nodes to connect;\n",
    "SPARK_MASTER_PORT is the master node port used by worker nodes to connect;\n",
    "PYSPARK_PYTHON is the installed Python location used by Apache Spark to support its Python API"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Composing the cluster\n",
    " \n",
    "The Docker compose file contains the recipe for our cluster. Here, we will create the JuyterLab and Spark nodes containers, expose their ports for the localhost network and connect them to the simulated HDFS.\n",
    "\n",
    "\n",
    "We start by creating the Docker volume for the simulated HDFS. \n",
    "\n",
    "Next, we create one container for each cluster component. \n",
    "\n",
    "The jupyterlab container exposes the IDE port and binds its shared workspace directory to the HDFS volume.\n",
    "\n",
    "Likewise, the spark-master container exposes its web UI port and its master-worker connection port and also binds to the HDFS volume.\n",
    "\n",
    "We finish by creating two Spark worker containers named spark-worker-1 and spark-worker-2. \n",
    "\n",
    "Each container exposes its web UI port (mapped at 8081 and 8082 respectively) and binds to the HDFS volume. \n",
    "\n",
    "These containers have an environment step that specifies their hardware allocation:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}