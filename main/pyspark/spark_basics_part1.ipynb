{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Spark and PySpark Basics\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Install The Spark and Hadoop together"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "from \n",
    "https://medium.com/@josemarcialportilla/getting-spark-python-and-jupyter-notebook-running-on-amazon-ec2-dec599e1c297"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "okay so from the instructions above, I wanted to do without anaconda and also I wanted to do it in container:\n",
    "\n",
    "\n",
    "So these basic changes\n",
    "\n",
    "```\n",
    "ENV SPARK_HOME='/spark-2.0.0-bin-hadoop2.7' \n",
    "ENV   PATH=$SPARK_HOME:$PATH \n",
    "ENV  PYTHONPATH=$SPARK_HOME/python:$PYTHONPATH\n",
    "```\n",
    "\n",
    "also [here](https://stackoverflow.com/questions/33379393/docker-env-vs-run-export) tells how to use export type commands in dockerfile\n",
    "\n",
    "but I got this error that with some search I realized that pyspark is not supporting the python 3.6 and above\n",
    "\n",
    "https://stackoverflow.com/questions/42349980/unable-to-run-pyspark\n",
    "\n",
    "https://issues.apache.org/jira/browse/SPARK-19019\n",
    "\n",
    "\n",
    "So from here my thought was that okay I will switch to the python 3.5\n",
    "\n",
    "(going back to the link above it seems that they claim to have fixed that in a later version of pyspark, so this is another route that I could take\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## install later version of pyspark"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "So taking a fresh look of this and seeing that the issue has been old and calimed to be resolved, I come to this basic idea \n",
    "that I could simply use a later version of spark, even better why not just find a later more credible source for installation.\n",
    "\n",
    "for example the spark official site!\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/getting_started/install.html\n",
    "\n",
    "Which says you could simply use pip install for that !!!\n",
    "\n",
    "Now since I had so much pain trying to figure this puzzle out, I would not simply go for that option, at least not just yet!\n",
    "plus not sure how flexible is that pip install option, so why not continue with manual installation, huh?!!\n",
    "\n",
    "https://spark.apache.org/downloads.html\n",
    "\n",
    "\n",
    "which gives me this link for the latest version \n",
    "\n",
    "https://www.apache.org/dyn/closer.lua/spark/spark-3.1.1/spark-3.1.1-bin-hadoop2.7.tgz\n",
    "\n",
    "which links to this \n",
    "\n",
    "https://downloads.apache.org/spark/spark-3.1.1/spark-3.1.1-bin-hadoop2.7.tgz\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "and it works!\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Simnply use pip install!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Install python 3.5\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "okay I wanted to simply install the python 3.5 using the apt-get, should be pretty simple huh? except it wasn't!\n",
    "well I with a lot of different suggestions I could get python 3.5 installed, but it seemed that it lacked zlib\n",
    "which the question of how to install python 3.5 with zlib was something I was struggling for long!\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}