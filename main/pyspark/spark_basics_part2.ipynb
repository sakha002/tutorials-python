{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Hadoop and spark "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "https://spark.apache.org/faq.html\n",
    "\n",
    "\n",
    "\n",
    "Spark is a fast and general processing engine compatible with Hadoop data. \n",
    "\n",
    "It can run in Hadoop clusters through YARN or Spark's standalone mode, \n",
    "\n",
    "and it can process data in HDFS, HBase, Cassandra, Hive, and any Hadoop InputFormat. \n",
    "\n",
    "It is designed to perform both batch processing (similar to MapReduce) and new workloads like streaming, interactive queries, and machine learning.\n",
    "\n",
    "\n",
    "You can use either the standalone deploy mode, which only needs Java to be installed on each node, or the Mesos and YARN cluster managers. \n",
    "\n",
    "If you'd like to run on Amazon EC2, AMPLab provides EC2 scripts to automatically launch a cluster.\n",
    "\n",
    "Note that you can also run Spark locally (possibly on multiple cores) without any special setup by just passing local[N] as the master URL, where N is the number of parallel threads you want.\n",
    "\n",
    "\n",
    "Do I need Hadoop to run Spark?\n",
    "\n",
    "No, but if you run on a cluster, you will need some form of shared file system (for example, NFS mounted at the same path on each node). If you have this type of filesystem, you can just deploy Spark in standalone mode.\n",
    "\n",
    "NFS, or Network File System, was designed in 1984 by Sun Microsystems. This distributed file system protocol allows a user on a client computer to access files over a network in the same way they would access a local storage file. \n",
    "\n",
    "To access data stored on another machine (i.e. a server) the server would implement NFS daemon processes to make data available to clients. The server administrator determines what to make available and ensures it can recognize validated clients.\n",
    "\n",
    "From the client's side, the machine requests access to exported data, typically by issuing a mount command. If successful, the client machine can then view and interact with the file systems within the decided parameters.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# The Spark Cluster with Hadoop cluster from Europe big data group"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "from https://github.com/big-data-europe/docker-hadoop-spark-workbench/"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "This one is too complicated so I will try to create a container and set-up the repos etc in that container."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "so originally as seen above I wanted to setup all of this image building and cluster setup within a container, since it seems that it needed so additional package installations on the env.\n",
    " So this is esssentially entering the world of docker in docker that is really meeded up (same as it is the case for inception movie!)\n",
    "\n",
    "So I found this docker in docker image that I thought okay that is what I need !\n",
    "\n",
    "https://hub.docker.com/_/docker\n",
    "\n",
    "\n",
    "https://github.com/docker-library/docker/issues/306\n",
    "\n",
    "(need to look into this later)\n",
    "\n",
    "but working a bit with it I realized that you don't get a full docker, or a full environment that you can do all docker capabailities. (well more on this to be found!)\n",
    "\n",
    "The other stupid thing I was trying to do was using the apt-get for installing packages. \n",
    "When I tried to build on top of that image, I was surprized that the apt-get does not work, and making the effort to install apt-get somehow, or yum , was hopeless of course. then to realize that this docker image was built essentially on a different kind of linux, alpine, which has its own package manager -> apk\n",
    "\n",
    "how to install apt-get in a container\n",
    "how to add apt-get to alpine\n",
    "\n",
    "well to be fair I was not the only one who do this rookie mistake:\n",
    "\n",
    "https://stackoverflow.com/questions/53389749/getting-apt-get-on-an-alpine-container\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "okay looks like the hadoop server is not working like expected :/\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Starting from pre-packed images\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "okay so I ran the docker compose for the pre-built images of the bde , group and I just added the jupyterlab image to the cluster and I was able to get it work. now the question is would I be able to set the same cluster up with the dockerfile?\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "okay first let's double check that the versions of the hadoop and spark match\n",
    "\n",
    "\n",
    "- building hadoop 3 images for 3.2.0 \n",
    "- building spark base, master, worker, and jupyterlab for spark 3.1.1 and hadoop 3.2\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}